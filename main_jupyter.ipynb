{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "is_start_model = False\n",
    "is_by_symbol = True\n",
    "is_plot_scatter = False\n",
    "is_plot_line = True\n",
    "\n",
    "days_backtracked = 7  # also known as batch\n",
    "train_ratio = 0.8\n",
    "num_of_features = 9\n",
    "\n",
    "initial_label = 0\n",
    "end_label = 4\n",
    "num_of_labels = end_label - initial_label\n",
    "\n",
    "LSTM_units = 64\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "optimizer_function = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "num_of_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    dataset = pd.read_parquet(filename)\n",
    "    return dataset\n",
    "\n",
    "def clean_data(dataframe):\n",
    "    cleaned_dataframe = dataframe.copy(deep=True)\n",
    "    cleaned_dataframe = cleaned_dataframe[cleaned_dataframe.Symbol != \"CEG\"]\n",
    "    cleaned_dataframe = cleaned_dataframe[cleaned_dataframe.Symbol != \"OGN\"]\n",
    "    cleaned_dataframe = cleaned_dataframe.drop([\"Security\", \"GICS Sub-Industry\"], axis=1)\n",
    "    cleaned_dataframe = cleaned_dataframe.reset_index()\n",
    "\n",
    "    column_to_move = cleaned_dataframe.pop(\"Symbol\")\n",
    "    cleaned_dataframe.insert(0, \"Symbol\", column_to_move)\n",
    "\n",
    "    column_to_move = cleaned_dataframe.pop(\"GICS Sector\")\n",
    "    cleaned_dataframe.insert(0, \"Sector\", column_to_move)\n",
    "\n",
    "    return cleaned_dataframe\n",
    "\n",
    "\n",
    "def process_data(dataframe):\n",
    "    grouped_by_symbol = dataframe.groupby(dataframe[\"Symbol\"])\n",
    "\n",
    "    dataframe_list_by_symbol = []\n",
    "    for name, data in grouped_by_symbol:\n",
    "        dataframe_list_by_symbol.append(data)\n",
    "\n",
    "    for i in range(len(dataframe_list_by_symbol)):\n",
    "        dataframe_list_by_symbol[i] = dataframe_list_by_symbol[i].reset_index(drop=True)\n",
    "        for j in range(days_backtracked, 0, -1):\n",
    "            dataframe_list_by_symbol[i][\"D-\" + str(j) + \" Open\"] = dataframe_list_by_symbol[i][\"Open\"].shift(periods=j, axis=0)\n",
    "            dataframe_list_by_symbol[i][\"D-\" + str(j) + \" High\"] = dataframe_list_by_symbol[i][\"High\"].shift(periods=j, axis=0)\n",
    "            dataframe_list_by_symbol[i][\"D-\" + str(j) + \" Low\"] = dataframe_list_by_symbol[i][\"Low\"].shift(periods=j, axis=0)\n",
    "            dataframe_list_by_symbol[i][\"D-\" + str(j) + \" Close\"] = dataframe_list_by_symbol[i][\"Close\"].shift(periods=j, axis=0)\n",
    "            dataframe_list_by_symbol[i][\"D-\" + str(j) + \" Adj Close\"] = dataframe_list_by_symbol[i][\"Adj Close\"].shift(periods=j, axis=0)\n",
    "            dataframe_list_by_symbol[i][\"D-\" + str(j) + \" Vol\"] = dataframe_list_by_symbol[i][\"Volume\"].shift(periods=j, axis=0)\n",
    "            dataframe_list_by_symbol[i][\"D-\" + str(j) + \" News Vol Proportion\"] = (dataframe_list_by_symbol[i][\"News - Volume\"] / dataframe_list_by_symbol[i][\"News - All News Volume\"]).shift(periods=j, axis=0)\n",
    "            dataframe_list_by_symbol[i][\"D-\" + str(j) + \"Pos News\"] = dataframe_list_by_symbol[i][\"News - Positive Sentiment\"].shift(periods=j, axis=0)\n",
    "            dataframe_list_by_symbol[i][\"D-\" + str(j) + \"Neg News\"] = dataframe_list_by_symbol[i][\"News - Negative Sentiment\"].shift(periods=j, axis=0)\n",
    "        dataframe_list_by_symbol[i].dropna(inplace=True, ignore_index=True)\n",
    "\n",
    "    return dataframe_list_by_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain scaling parameters from training and scale entire data per company\n",
    "def normalize_by_symbol(dataframe_list_by_symbol):\n",
    "    num_of_companies = len(dataframe_list_by_symbol)\n",
    "    num_of_rows = len(dataframe_list_by_symbol[0])\n",
    "    num_of_train_rows = int(train_ratio * num_of_rows)\n",
    "\n",
    "    df_copy_list = []\n",
    "    for i in range(num_of_companies):\n",
    "        df_copy_list.append(dataframe_list_by_symbol[i].copy(deep=True))\n",
    "\n",
    "    for i in range(num_of_companies):\n",
    "        for j in range(25, len(df_copy_list[i].columns)):\n",
    "            col_min = df_copy_list[i].iloc[:num_of_train_rows, j].min()\n",
    "            col_max = df_copy_list[i].iloc[:num_of_train_rows, j].max()\n",
    "            diff = col_max - col_min\n",
    "            if diff != 0:\n",
    "                df_copy_list[i].iloc[:, j] -= col_min\n",
    "                df_copy_list[i].iloc[:, j] /= diff\n",
    "\n",
    "    return df_copy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_symbol(dataframe):\n",
    "    grouped_by_symbol = dataframe.groupby(dataframe[\"Symbol\"])\n",
    "    dataframe_list_by_symbol = []\n",
    "    for name, data in grouped_by_symbol:\n",
    "        dataframe_list_by_symbol.append(data)\n",
    "\n",
    "    return dataframe_list_by_symbol\n",
    "\n",
    "\n",
    "def group_by_sector(dataframe):\n",
    "    grouped_by_sector = dataframe.groupby(dataframe[\"Sector\"])\n",
    "    dataframe_list_by_sector = []\n",
    "    for name, data in grouped_by_sector:\n",
    "        dataframe_list_by_sector.append(data)\n",
    "\n",
    "    return dataframe_list_by_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dfs by symbol -> copy -> combined df -> list of df by sector -> list of train df by sector -> scaling -> recombine -> list of dfs by symbol\n",
    "def normalize_by_sector(dataframe_list_by_symbol):\n",
    "    num_of_companies = len(dataframe_list_by_symbol)\n",
    "    num_of_rows = len(dataframe_list_by_symbol[0])\n",
    "    num_of_train_rows = int(train_ratio * num_of_rows)\n",
    "\n",
    "    df_copy_list = []\n",
    "    for i in range(num_of_companies):\n",
    "        df_copy_list.append(dataframe_list_by_symbol[i].copy(deep=True))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(num_of_companies):\n",
    "        df = pd.concat([df, df_copy_list[i]])\n",
    "\n",
    "    grouped_by_sector = group_by_sector(df)\n",
    "    num_of_sectors = len(grouped_by_sector)\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    for i in range(num_of_sectors):\n",
    "        train_df = pd.concat([train_df, grouped_by_sector[i].iloc[:num_of_train_rows, :]])\n",
    "\n",
    "    train_grouped_by_sector = group_by_sector(train_df)\n",
    "    for i in range(num_of_sectors):\n",
    "        for j in range(25, len(train_grouped_by_sector[i].columns)):\n",
    "            col_min = train_grouped_by_sector[i].iloc[:, j].min()\n",
    "            col_max = train_grouped_by_sector[i].iloc[:, j].max()\n",
    "            diff = col_max - col_min\n",
    "            if diff != 0:\n",
    "                grouped_by_sector[i].iloc[:, j] -= col_min\n",
    "                grouped_by_sector[i].iloc[:, j] /= diff\n",
    "\n",
    "    recombined_df = pd.DataFrame()\n",
    "    for i in range(num_of_sectors):\n",
    "        recombined_df = pd.concat([recombined_df, grouped_by_sector[i]])\n",
    "\n",
    "    return group_by_symbol(recombined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(normalized_dataframe_list):\n",
    "    num_of_companies = len(normalized_dataframe_list)\n",
    "    num_of_rows = len(normalized_dataframe_list[0])\n",
    "    num_of_train_rows = int(train_ratio * num_of_rows)\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    for i in range(num_of_companies):\n",
    "        train_df = pd.concat([train_df, normalized_dataframe_list[i].iloc[:num_of_train_rows, :]])\n",
    "        test_df = pd.concat([test_df, normalized_dataframe_list[i].iloc[num_of_train_rows:, :]])\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def convert_to_lstm_input(dataset_single):\n",
    "    dataset_single = dataset_single.iloc[:, 3:]\n",
    "    dataset_single_as_np = dataset_single.to_numpy()\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    num_of_rows = len(dataset_single_as_np)\n",
    "    for i in range(num_of_rows):\n",
    "        features_by_day = []\n",
    "        for j in range(days_backtracked):\n",
    "            offset = j * num_of_features\n",
    "            features_by_day.append(dataset_single_as_np[i, (22 + offset):(22 + num_of_features + offset)])\n",
    "        x.append(features_by_day)\n",
    "        y.append(dataset_single_as_np[i, initial_label:end_label])\n",
    "\n",
    "    x_as_np = np.array(x)\n",
    "    y_as_np = np.array(y)\n",
    "\n",
    "    return x_as_np, y_as_np\n",
    "\n",
    "\n",
    "def create_model(shape):\n",
    "    lstm_model = tf.keras.models.Sequential()\n",
    "\n",
    "    lstm_model.add(tf.keras.layers.InputLayer(input_shape=shape))\n",
    "\n",
    "    lstm_model.add(tf.keras.layers.LSTM(units=LSTM_units, return_sequences=True))\n",
    "    lstm_model.add(tf.keras.layers.LSTM(units=LSTM_units))\n",
    "    lstm_model.add(tf.keras.layers.Dense(units=LSTM_units, kernel_initializer=\"lecun_normal\", activation=\"selu\"))\n",
    "\n",
    "    lstm_model.add(tf.keras.layers.Dense(units=num_of_labels, activation=\"linear\"))\n",
    "\n",
    "    return lstm_model\n",
    "\n",
    "\n",
    "def fit_model(model, x_train, y_train):\n",
    "    lstm_cp = tf.keras.callbacks.ModelCheckpoint(\"best_model/\", save_best_only=True)\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optimizer_function, metrics=tf.keras.metrics.RootMeanSquaredError())\n",
    "    history = model.fit(x_train, y_train, validation_split=0.1, epochs=num_of_epochs, callbacks=[lstm_cp])\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def fit_model_industry(model, x_train, y_train, rows_per_company):\n",
    "    lstm_cp = tf.keras.callbacks.ModelCheckpoint(\"best_model_industry/\", save_best_only=True)\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optimizer_function, metrics=tf.keras.metrics.RootMeanSquaredError())\n",
    "\n",
    "    history = None\n",
    "    train_rows_per_company = int(train_ratio * rows_per_company)\n",
    "    num_of_companies = int(len(x_train)/train_rows_per_company)\n",
    "    for i in range(num_of_companies):\n",
    "        initial = i * train_rows_per_company\n",
    "        end = initial + train_rows_per_company\n",
    "        history = model.fit(x_train[initial:end], y_train[initial:end], validation_split=0.1, epochs=num_of_epochs, callbacks=[lstm_cp])\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def compare_predictions(model, x, y):\n",
    "    predictions = model.predict(x)\n",
    "    print(predictions)\n",
    "\n",
    "    if is_plot_scatter:\n",
    "        plt.scatter(predictions, y)\n",
    "        plt.xlabel(\"predicted\")\n",
    "        plt.ylabel(\"actual\")\n",
    "        plt.axline((0, 0), slope=1)\n",
    "        plt.show()\n",
    "\n",
    "    if is_plot_line:\n",
    "        x_axis = np.arange(len(predictions))\n",
    "        labels_list = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "        enumerated_labels = enumerate(labels_list)\n",
    "        for i, label in enumerated_labels:\n",
    "            plt.plot(x_axis, predictions[:, i], label=label)\n",
    "            plt.plot(x_axis, y[:, i], label=\"Actual \" + label)\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filename = None # change this\n",
    "    raw_dataframe = load_data(filename)\n",
    "    cleaned_dataframe = clean_data(raw_dataframe)\n",
    "    dataframe_list_by_symbol = process_data(cleaned_dataframe)\n",
    "\n",
    "    if is_by_symbol:\n",
    "        normalized_dataframe_list = normalize_by_symbol(dataframe_list_by_symbol)\n",
    "    else:\n",
    "        normalized_dataframe_list = normalize_by_sector(dataframe_list_by_symbol)\n",
    "\n",
    "    train_df, test_df = split_train_test(normalized_dataframe_list)\n",
    "\n",
    "    train_df_list_by_symbol = group_by_symbol(train_df)\n",
    "    test_df_list_by_symbol = group_by_symbol(test_df)\n",
    "    train_df_list_by_sector = group_by_sector(train_df)\n",
    "    test_df_list_by_sector = group_by_sector(test_df)\n",
    "\n",
    "    rows_per_company = len(dataframe_list_by_symbol[0])\n",
    "    num_of_test_rows_per_company = int((1 - train_ratio)*rows_per_company)\n",
    "\n",
    "    if is_start_model:\n",
    "        if is_by_symbol:\n",
    "            x_train, y_train = convert_to_lstm_input(train_df_list_by_symbol[117])\n",
    "            input_shape = (np.shape(x_train)[1], np.shape(x_train)[2])\n",
    "            lstm_model = create_model(input_shape)\n",
    "            results = fit_model(lstm_model, x_train, y_train)\n",
    "\n",
    "            x_test, y_test = convert_to_lstm_input(test_df_list_by_symbol[117])\n",
    "            best_model = tf.keras.models.load_model(\"best_model/\")\n",
    "            compare_predictions(best_model, x_test, y_test)\n",
    "\n",
    "        else:\n",
    "            x_train, y_train = convert_to_lstm_input(train_df_list_by_sector[2])\n",
    "            input_shape = (np.shape(x_train)[1], np.shape(x_train)[2])\n",
    "            lstm_model = create_model(input_shape)\n",
    "            results = fit_model_industry(lstm_model, x_train, y_train, rows_per_company)\n",
    "\n",
    "            x_test, y_test = convert_to_lstm_input(test_df_list_by_sector[2])\n",
    "            best_model = tf.keras.models.load_model(\"best_model_industry/\")\n",
    "            compare_predictions(best_model, x_test[:num_of_test_rows_per_company], y_test[:num_of_test_rows_per_company])\n",
    "\n",
    "        print(min(results.history[\"val_root_mean_squared_error\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m# change this\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     raw_dataframe \u001b[39m=\u001b[39m load_data(filename)\n\u001b[1;32m      4\u001b[0m     cleaned_dataframe \u001b[39m=\u001b[39m clean_data(raw_dataframe)\n\u001b[1;32m      5\u001b[0m     dataframe_list_by_symbol \u001b[39m=\u001b[39m process_data(cleaned_dataframe)\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m(filename):\n\u001b[0;32m----> 2\u001b[0m     dataset \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(filename)\n\u001b[1;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parquet.py:501\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39m@doc\u001b[39m(storage_options\u001b[39m=\u001b[39m_shared_docs[\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    448\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_parquet\u001b[39m(\n\u001b[1;32m    449\u001b[0m     path: FilePath \u001b[39m|\u001b[39m ReadBuffer[\u001b[39mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    455\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m    456\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39m    DataFrame\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m     impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    503\u001b[0m     \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39mread(\n\u001b[1;32m    504\u001b[0m         path,\n\u001b[1;32m    505\u001b[0m         columns\u001b[39m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    509\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parquet.py:52\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     50\u001b[0m             error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(err)\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to find a usable engine; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtried using: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfastparquet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA suitable version of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msupport.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to import the above resulted in these errors:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00merror_msgs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
